{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b11c521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for POI_FINAL_Museum.csv:\n",
      "                Name   Latitude  Longitude  Weighted Sum  Rank\n",
      "0            Jane A.  40.767370 -73.981125      0.018420   8.0\n",
      "1           Spyscape  40.765198 -73.983618      0.017915   9.0\n",
      "2         The Africa  40.796289 -73.949378      0.012742  10.0\n",
      "3    Madame Tussauds  40.756539 -73.988521      0.028508   7.0\n",
      "4           New York  40.752895 -73.977589      0.071232   5.0\n",
      "5         Center for  40.752768 -73.992304      0.039571   6.0\n",
      "6  Fashion Institute  40.747304 -73.994797      0.169219   3.0\n",
      "7       Hebrew Union  40.728807 -73.994549      0.136504   4.0\n",
      "8         Lower East  40.724864 -73.992735      0.449736   1.0\n",
      "9            The New  40.726021 -73.999809      0.309176   2.0\n",
      "\n",
      "\n",
      "Results for POI_FINAL_Restaurant.csv:\n",
      "                    Name   Latitude  Longitude  Weighted Sum  Rank\n",
      "0       Daily Provisions  40.782225 -73.978646      0.023968   7.0\n",
      "1          Levain Bakery  40.781511 -73.979099      0.013476  10.0\n",
      "2            Sushi Kaito  40.779232 -73.983555      0.019630   8.0\n",
      "3                 By The  40.778060 -73.956894      0.013667   9.0\n",
      "4          Levain Bakery  40.777395 -73.955514      0.059058   6.0\n",
      "5          Breads Bakery  40.771077 -73.981750      0.074803   5.0\n",
      "6          Empanada Mama  40.764419 -73.988481      0.091570   4.0\n",
      "7            Bo's Bagels  40.804260 -73.954145      0.117187   3.0\n",
      "8  UrbanSpace Vanderbilt  40.754438 -73.976426      0.204322   2.0\n",
      "9       Boqueria Spanish  40.755471 -73.990095      0.380082   1.0\n",
      "\n",
      "\n",
      "Results for POI_FINAL_Travel_Agency.csv:\n",
      "                      Name   Latitude  Longitude  Weighted Sum  Rank\n",
      "0               DFC Travel  40.787374 -73.975875      0.005577  10.0\n",
      "1                KG Travel  40.773881 -73.947643      0.026889   9.0\n",
      "2                  Hotwire  40.763570 -73.985957      0.033909   8.0\n",
      "3        BlueOrange Travel  40.762041 -73.984630      0.039335   7.0\n",
      "4  Protravel International  40.759743 -73.973793      0.077163   5.0\n",
      "5        Gad International  40.756935 -73.977877      0.064855   6.0\n",
      "6         Empire Limousine  40.757123 -73.986884      0.088176   4.0\n",
      "7                 Certares  40.754604 -73.978267      0.184141   3.0\n",
      "8         Executive Global  40.757527 -73.989914      0.293913   1.0\n",
      "9              Encore Jets  40.755089 -73.986280      0.250356   2.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = ['POI_FINAL_Museum.csv', 'POI_FINAL_Restaurant.csv', 'POI_FINAL_Travel_Agency.csv']\n",
    "\n",
    "# Define weights for each criterion\n",
    "weights = {\n",
    "    'Distance': 0.2,\n",
    "    'Popularity': 0.1,\n",
    "    'Ratings': 0.3,\n",
    "    'Total Ratings': 0.1,\n",
    "    'Price($)': 0.3\n",
    "}\n",
    "\n",
    "# Define the pairwise comparison matrix for criteria\n",
    "criteria_comparison = {\n",
    "    'Distance': [1, 3, 5, 7, 9],\n",
    "    'Popularity': [1/3, 1, 3, 5, 7],\n",
    "    'Ratings': [1/5, 1/3, 1, 3, 5],\n",
    "    'Total Ratings': [1/7, 1/5, 1/3, 1, 3],\n",
    "    'Price($)': [1/9, 1/7, 1/5, 1/3, 1]\n",
    "}\n",
    "\n",
    "for file in csv_files:\n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file, encoding='latin1')\n",
    "    \n",
    "    # Normalize the data\n",
    "    for criterion in weights.keys():\n",
    "        if criterion == 'Price($)':\n",
    "            data[criterion] = (data[criterion].max() - data[criterion]) / (data[criterion].max() - data[criterion].min())\n",
    "        else:\n",
    "            data[criterion] = (data[criterion] - data[criterion].min()) / (data[criterion].max() - data[criterion].min())\n",
    "    \n",
    "    # Calculate the weighted sum using AHP\n",
    "    weighted_sum = np.zeros(len(data))\n",
    "    for criterion, weight in weights.items():\n",
    "        pairwise_comparison_vector = np.array(criteria_comparison[criterion])\n",
    "        normalized_comparison_vector = pairwise_comparison_vector / np.sum(pairwise_comparison_vector)\n",
    "        normalized_comparison_vector_extended = np.repeat(normalized_comparison_vector, len(data) // len(normalized_comparison_vector))\n",
    "        weighted_sum += data[criterion].values * normalized_comparison_vector_extended * weight\n",
    "\n",
    "    \n",
    "    # Add the weighted sums to the DataFrame\n",
    "    data['Weighted Sum'] = weighted_sum\n",
    "    \n",
    "    # Rank the alternatives based on the weighted sum\n",
    "    data['Rank'] = data['Weighted Sum'].rank(ascending=False, method='min')\n",
    "    \n",
    "    # Display the result\n",
    "    print(f\"Results for {file}:\")\n",
    "    # Truncate the name\n",
    "    data['Name'] = data['Name'].apply(lambda x: ' '.join(x.split()[:2]))\n",
    "    print(data[['Name', 'Latitude', 'Longitude', 'Weighted Sum', 'Rank']])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d27903e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
